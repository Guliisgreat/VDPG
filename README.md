# VDPG: Adapting to Distribution Shift by Visual Domain Prompt Generation (ICLR 2024)
[Paper](https://openreview.net/forum?id=sSaN4gxuEf&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions))  / [Project](https://chi-chi-zx.github.io/VDPG_ICLR24/)

## ðŸ’¡ Abstract
In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains. Previous studies fail to utilize recent foundation models with strong out-of-distribution generalization. Additionally, domain-centric designs are not flavored in their works. Furthermore, they employ the process of modelling source domains and the process of learning to adapt independently into disjoint training stages. In this work, we propose an approach on top of the pre-computed features of the foundation model. Specifically, we build a knowledge bank to learn the transferable knowledge from source domains. Conditioned on few-shot target data, we introduce a domain prompt generator to condense the knowledge bank into a domain-specific prompt. The domain prompt then directs the visual features towards a particular domain via a guidance module. Moreover, we propose a domain-aware contrastive loss and employ meta-learning to facilitate domain knowledge extraction. Extensive experiments are conducted to validate the domain knowledge extraction. The proposed method outperforms previous work on 5 large-scale benchmarks including WILDS and DomainNet.

## ðŸ’¡ News
- [2024/05] [Paper]() is on Arxiv.
- [2024/05] VDPG is released.
- [2024/01] VDPG has been accepted by ICLR 2024

## Installation
### Requirements
```bash
pip install -r torch21_cu118_py39.yml
```

### Download CLIP's pretrained weights
From [open_clip](https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md) to 
```bash
./modelzoo/openai_clip
```
### Download datasets
**WILDS**

Please follow the download instructions provided by [WILDS benchmark](https://github.com/p-lambda/wilds/) to download iWildCam, Camelyon17, FMoW and ProvertyMap to the folder of `./data`

**DomainNet**

1. Download the official [DomainNet benchmark](http://ai.bu.edu/M3SDA/) to `./data`
2. Generate a metadata.csv for DomainNet by running the [WILDS preprocessing script](https://github.com/p-lambda/wilds/blob/472677590de351857197a9bf24958838c39c272b/dataset_preprocessing/domainnet/generate_metadata.py).

Noted, we evaluate the **`official test split`** of DomainNet instead of the random split of the target domain as in [DomainBed](https://github.com/facebookresearch/DomainBed) codebase.

## Project Structure
Reference: [lightning-hydra-template](https://github.com/ashleve/lightning-hydra-template/)

```
â”œâ”€â”€ configs                   <- Hydra configs
â”‚   â”œâ”€â”€ callbacks                <- Callbacks configs
â”‚   â”œâ”€â”€ data                     <- Data configs
â”‚   â”œâ”€â”€ experiment               <- Experiment configs
â”‚   â”œâ”€â”€ extras                   <- Extra utilities configs
â”‚   â”œâ”€â”€ logger                   <- Logger configs
â”‚   â”œâ”€â”€ model                    <- Model configs
â”‚   â”œâ”€â”€ paths                    <- Project paths configs
â”‚   â”œâ”€â”€ trainer                  <- Trainer configs
â”‚   â”œâ”€â”€ eval.yaml             <- Main config for evaluation
â”‚   â””â”€â”€ train.yaml            <- Main config for training
â”‚
â”œâ”€â”€ logs                   <- Logs generated by hydra and lightning loggers
â”œâ”€â”€ modelzoo               <- pretrained model weights
â”œâ”€â”€ data                   <- downloaded datasets
â”œâ”€â”€ src                    <- Source code
â”‚   â”œâ”€â”€ datasets                <- benchmark datasets
â”‚   â”œâ”€â”€ models                  <- model components
â”‚   â”œâ”€â”€ lightning               <- LightningModule, DataModule, Callbacks
â”‚   â”œâ”€â”€ solver                  <- losses, solvers, schedulers
â”‚   â”œâ”€â”€ utils                   <- Utility modules
â”œâ”€â”€ train.py                    <- Run training
â”œâ”€â”€ eval.py                     <- Run evaluation
â”‚
â”œâ”€â”€ .env                      <- Example of file for storing private environment variables
â”œâ”€â”€ .project-root             <- File for inferring the position of project root directory
â”œâ”€â”€ requirements.yml          <- File for installing pip environment
â””â”€â”€ README.md
```
<br>

## Evaluation
### Download pretrained checkpoints 
From [OneDrive](https://utoronto-my.sharepoint.com/:f:/g/personal/zhixiang_chi_mail_utoronto_ca/EuOsVq42nAlCvLm2_F3se5wB6XcBIKIaHO2YKKKxMPoMhQ?e=s6Whcp) and save in the folder of `./modelzoo`

### DomainNet
```bash
python eval.py model=vdpg_ViT_B16_CLIP.yaml paths.data_dir="./data" data=<data_name> ckpt_path=./modelzoo/<ckpt_name>
```

For example, we run the evaluation using DomainNet's sketch domain as the out-of-distribution.
```bash
python eval.py model=vdpg_ViT_B16_CLIP.yaml paths.data_dir="./data" data=domainnet_sketch_contrastive.yaml ckpt_path=./modelzoo/domainnet_sketch.ckpt
```
### WILDS
iWildCam: 
``` bash
python eval.py model.model.num_prompts=100 paths.data_dir="./data" data=iwild_contrastive ckpt_path=./modelzoo/iWildCam.ckpt
```

Camelyon: 
``` bash
python eval.py model.model.num_prompts=5 paths.data_dir="./data" data=camelyon17_contrastive ckpt_path=./modelzoo/Camelyon.ckpt
```

FMoW: 
``` bash
python eval.py model.model.num_prompts=5 paths.data_dir="./data" data=fmow_contrastive ckpt_path=./modelzoo/FMoW.ckpt
```

### Reproduced results using pretrained checkpoints 
|                     | OOD Top1 Acc |  Others      |
|:-------------------:|:------------:|:------------:|
|       iwildcam      |    0.7898    |    OOD F1-score: 0.4678    |
|         fmow        |    0.6235    |    OOD WR acc: 0.4689    |
|       camelyon      |    0.9604    |       -      |
|  DomainNet clipart  |    0.7643    |       -      |
| DomainNet infograph |    0.4923    |       -      |
|   DomainNet paint   |    0.6792    |       -      |
|   DomainNet quick   |    0.1756    |       -      |
|    DomainNet real   |    0.8182    |       -      |
|   DomainNet sketch  |    0.6681    |       -      |


## Train (To be added)

## <a name="cite"/> :clipboard: Citation

If you use this code in your research, please consider citing our paper:
```
@inproceedings{chi_2024_ICLR,
  title={Adapting to Distribution Shift by Visual Domain Prompt Generation},
  author={Zhixiang Chi, Li Gu, Tao Zhong, Huan Liu, Yuanhao Yu, Konstantinos N Plataniotis, Yang Wang},
  booktitle={Proceedings of the Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{
zhong2022metadmoe,
title={Meta-{DM}oE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts},
author={Tao Zhong and Zhixiang Chi and Li Gu and Yang Wang and YUANHAO YU and Jin Tang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=_ekGcr07Dsp}
}
```
